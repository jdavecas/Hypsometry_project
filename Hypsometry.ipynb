{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "from pathlib import Path\n",
    "import folium\n",
    "from folium import plugins\n",
    "import mapclassify\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import glob \n",
    "import plotly.graph_objects as go\n",
    "import fiona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the current working directory\n",
    "#os.chdir(\"..\") Used only fto go down one directory\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List all the shapefiles in the directory and split them into nodes and reaches\n",
    "shpNd = glob.glob('SWOT_L2_HR_RiverSP/**/*Node*.shp', recursive=True) #Nodes\n",
    "shpRch = glob.glob('SWOT_L2_HR_RiverSP/**/*Reach*.shp', recursive=True) #Reaches\n",
    "shpNd,shpRch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shpNd_022_NA = glob.glob('SWOT_L2_HR_RiverSP/**/*Node*_022_NA*.shp', recursive=True) #Nodes\n",
    "shpNd_022_NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodataframes = []\n",
    "for i in range(len(shpNd_022_NA)):\n",
    "   geodataframes.append(gpd.read_file(shpNd_022_NA[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetted_dataframes = []\n",
    "\n",
    "# Loop over the list of dataframes\n",
    "for dataframe in geodataframes:\n",
    "\n",
    "    # Subset the dataframe to the desired columns and append to the list\n",
    "    subsetted_dataframes.append(dataframe[['node_id', 'lat', 'lon', 'river_name', 'wse', 'width']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetted_dataframes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Nodes022_NA)):\n",
    "    df[i] = Nodes022_NA[i].drop(Nodes022_NA[i][Nodes022_NA.width != -999999999999.0].index)\n",
    "\n",
    "#df[0].get('width').iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.read_file(Nodes[0]).get('width').iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames={}\n",
    "for i in range(len(shpNd_022_NA)):\n",
    "        frames[i] = pd.concat([gpd.read_file(shpNd_022_NA[i]).get('node_id'), gpd.read_file(shpNd_022_NA[i]).get('river_name'), gpd.read_file(shpNd_022_NA[i]).get('lat'), \n",
    "        gpd.read_file(shpNd_022_NA[i]).get('lon'), gpd.read_file(shpNd_022_NA[i]).get('wse'), gpd.read_file(shpNd_022_NA[i]).get('width')], axis=1)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_rows = [frames.iloc[0] for frames in frames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the shapefile. [0] is used to read the first shapefile in the list\n",
    "gpd.read_file(shpNd[1]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = gpd.read_file(shpNd[0]).explore('wse', cmap='viridis', legend=True)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWOT_HR_shp1 = gpd.read_file(shpNd[0])[['river_name','p_wse','p_width','geometry']]\n",
    "SWOT_HR_shp1 =SWOT_HR_shp1.rename(columns={'river_name': 'River Name','p_wse':'Water Surface Elevation','p_width':'River Width'})\n",
    "SWOT_HR_shp1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = SWOT_HR_shp1\n",
    "updated_m = L.explore(column='Water Surface Elevation', cmap='viridis', legend=True)\n",
    "\n",
    "updated_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.crs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"**/*\")\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes =   glob.glob(os.path.join('SWOT_L2_HR_RiverSP','*','*.shp'))\n",
    "list_of_dfs = []\n",
    "for f in shapes:\n",
    "    df = gpd.read_file(f)\n",
    "    list_of_dfs.append(df)\n",
    "\n",
    "list_of_dfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dfs[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hypsometry",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
